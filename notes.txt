
The Big FAQ (part 8) can be revisted if at any time I get stuck for easy solutions to common problems

Course Roadmap:
  Getting Requirements ✓
  Docker Install ✓
  Container Basics ✓
  Image Basics ✓
  Docker Networking ✓
  Docker Volumes ✓
  Docker Compose ✓
  Orchestration ✓
  Docker Swarm
  Kubernetes
  Swarm vs. K8s
  Student Q&A
  File Reviews

  References and Documentation all throughout
    Github:
    - Course Github Repo - https://github.com/bretfisher/udemy-docker-mastery
    - Github Basics and Docs - https://docs.github.com/en/get-started/quickstart/set-up-git
    Docker:
    - Docker Official Images Library - https://github.com/docker-library/official-images/tree/master/library
    - Storage Drivers Overview - https://docs.docker.com/storage/storagedriver/
    - Data Storage Overview - https://docs.docker.com/storage/
    Docker-Compose:
    - Docker Official Documentation for Docker-Compose - https://docs.docker.com/compose/compose-file/
    - YAML official site - https://yaml.org/
    - YAML formatting scribnotes - https://yaml.org/refcard.html
    Swarm:
    - Docker Official Documentation for swarms - https://docs.docker.com/engine/swarm/services/
    Articles:
    - Methodology for building software-as-a-service apps - https://12factor.net/
    Resources:
    - Static Site Generator - https://jekyllrb.com/

Why Docker?:
  Isolation
    - old servers kept apps all over the place spread across machines, causing regular problems, but then came...
    - Virtual Machines: VM's
      allowed servers to be spun up on demand, accessability became a problem as even simple programs contained uneccesary chaff
  Environment
    - programming for multiple environments bleeds complexity.
    - the "Matrix from Hell" represents every part of your app interacting with every user.
    - Containers: an abstract, representing the "MfH"
  Speed (of change)
    - discussing the speed of business, not anything to do with hardware or software
    - Containers are the next step in computing that allow faster; development, building, testing, and deploying

The 3 step Docker workflow:
  build
    - Dockerfile: OCI image standard
    - universal package manager
    Image layers
      python bins+libs
      flask
      app
  ship
    - Docker Registry: OCI distribution spec
    - universal app distribution
    - Built image has a unique SHA hash, this allows it to be shared
      push image (docker push)
      pull image (docker pull)
  run
    runs Docker to pull an image
    - Linux/Win Server: self contained machine
    - IMAGE:
      creates and runs container(s) containing your app
      allows mass hosting of the same page.

Running Containers:
  3 major ways to run containers
    - Locally: (Docker Desktop, RD)
    - Server: (Docker Engine, K8s)
    - PaaS: (Cloud Run, Fargate)

What is an Image?
  Quick definition:"The application binaries and dependencies for your app and the metadata on how to run it":
  Official definition:"An Image is an ordered collection of root filesystem changes and 
      the corresponding execution parameters for use witin a container runtime.":
  Importantly, an image is only the kernel modules, without the kernel. It is NOT an OS, which makes it much lighter than VMs
  Dockerhub:
    - basically Github for Docker images
    - best practice for production includes pulling exact versions of an image
    - Alpine images are typically smaller than their counterparts
  Layers:
    - Images are built on a layer system
    - you never store the same data more than once
    - each time your image uses the same layer as another image, it's using the actual same layer
  (docker inspect *image*) gives a json file containing all the metadata for the image
  Image, Tag and Push:
    {docker image tag *--help*}
    - tags point to a specific image version
    - when you pull IMAGE:latest or IMAGE:mainline you are pulling a specific version of IMAGE
    {docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]}
    - if you don't specify a tag it will default to "latest"
    - you can tag pretty much anything as pretty much anything, follow conventions please

Persistent Data
  Containers are built to be disposable and easy to change, swap, and redeploy.
  This can cause issues when we want data to stick around between deployments.
  The two solutions Docker makes use of are [Volumes] and [Bind Mounts]
    - Volumes: make a special location outside of the container UFS
    - Bind Mounts: Link the containers path to a host path
  Volumes:
    - VOLUME is a command in Dockerfile, the choosen path will reference a location outside the container created
    (VOLUME [/PATH])
    - when a container is removed the volume will remain and the data in it is safe
    - volumes can be named for human readability
    {docker container run [OPTIONS] IMAGE [-v NAME:/PATH] [ARG]}
  Bind Mounting:
    - Bind Mounting is when you map a host file or directory to a container file or directory
    - this is effectively just having two seperate pointers for the same location
    - Can't use in Dockerfile, must be set up at container run
    {... run [-v /HOST_DIRECTORY:/CONTAINER_PATH]}
    - in a folder which contains your index.html
    {docker container run -d --name IMAGE_NAME -p PORT_MAP [-v $(pwd):/CONTAINER_HTML_PATH] IMAGE}
    - the example used is
    {docker container run -d --name nginx -p 80:80 -v $(pwd):/usr/share/nginx/html nginx}
    - this allows you to edit your code on a host machine and have it apply to your containers without needing to work inside them

Docker Compose
  Containers are typically self-contained and designed for a single or small number of tasks
  Docker Compose makes it easier to manipulate data and processes through many containers working in tandem
  Docker Compose is comprised of two major parts, [YAML File] and [docker-compose CLI tool]
    - YAML-formatted files describe our solutions for a number of tasks relating to our container groups and display heiarchy
    - docker-compose CLI is used for manual manipulation of YAML parameters
  YAML:
    - containers
    - networks
    - volumes
  docker-compose:
    - development
    - test automation
  docker-compose is the default filename, but any filename can be used if you instead call docker-compose -f 'filename'
  docker-compose has been integrated with docker desktop, (docker compose up) is now a better version of (docker-compose up)

Swarm Mode
  ? How do we automate container lifecycle ?
  ? How can we easily scale directionally ?
  ? How can we ensure our containers restart on failure ?
  ? How can we replace containers without downtime ?
  ? How can we control and track where containers start ?
  ? How can we create cross-node virtual networks ?
  ? How can we ensure only secure servers run our containers ?
  ? How can we store secrets, keys, passwords and make sure only the correct container gets them ?
  ! INTRODUCING SWARM MODE !
  - Swarm Mode is a server clustering solution built into Docker
  - unrelated to Swarm "classic" for pre-1.12 versions
  Swarm allows the creation of a 'service' which can create and manage 'workers'.
    this allows a 'manager' to offload tasks to available 'worker' nodes and reduce individual workloads
  Swarm is disabled by default, once enabled you get new commands such as;
    (docker swarm), (docker node), (docker service), (docker stack), and (docker secret)
    To enable: use (docker swarm init). This creates a single-node swarm by default
  - Once swarm is enabled 'service' replaces 'compose' in general command use
    {docker service create alpine ping 8.8.8.8} #ping 8.8.8.8 is just to give image something to do
    {docker service ls}
    {docker service update [IMAGE_ID] --replicas 3} #creates copies of selected image
    {docker container ls} #now shows 3 identical containers
    - if an individual container fails then service will automatically recreate it.
    {docker service rm [IMAGE_NAME]} #required to actually remove containers
  Spinning up VM's to play with:
    Docker-Machine has been depreciated, use Multipass as a better option for managing multiple local VM's
    I intend to use "labs.play-with-docker.com", though you only have 4 hours with those VM's before they wipe
